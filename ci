#!/usr/bin/env -S uv run --script
# /// script
# requires-python = ">=3.12"
# dependencies = [
#     "click>=8.1.0",
#     "rich>=13.0.0",
# ]
# ///
"""Build and test tools for the Bazel test set rules project.

Container commands (build, test, check, query) can be run from the host
(auto-delegates into the container) or directly inside the container
(e.g. after ./ci shell).
"""

from __future__ import annotations

import fcntl
import grp
import os
import pwd
import shlex
import shutil
import subprocess
import sys
import time
from pathlib import Path

import click
from rich.console import Console

PROJECT_ROOT = Path(__file__).parent.resolve()
TARGET_DIR = PROJECT_ROOT / "target"
IMAGE_NAME = "test-sets-bazel-devenv"
DOCKERFILE = PROJECT_ROOT / "Dockerfile.devenv"
IN_CONTAINER = os.environ.get("IN_CONTAINER") == "1"

console = Console()


def run_cmd(
    cmd: list[str], check: bool = True, **kwargs
) -> subprocess.CompletedProcess:
    """Run a command with logging."""
    console.print(f"[dim]$ {' '.join(cmd)}[/dim]")
    try:
        return subprocess.run(cmd, check=check, text=True, **kwargs)
    except subprocess.CalledProcessError as e:
        console.print(f"[red]Command failed with exit code {e.returncode}[/red]")
        raise


def _docker_build_cmd(extra_args: list[str] | None = None) -> list[str]:
    """Return the base docker build command with UID/GID build args."""
    cmd = [
        "docker", "build",
        "--build-arg", f"HOST_UID={os.getuid()}",
        "--build-arg", f"HOST_GID={os.getgid()}",
        "--build-arg", f"HOST_USER={pwd.getpwuid(os.getuid()).pw_name}",
        "--build-arg", f"HOST_GROUP={grp.getgrgid(os.getgid()).gr_name}",
    ]
    if extra_args:
        cmd.extend(extra_args)
    cmd.extend(["-t", IMAGE_NAME, "-f", str(DOCKERFILE), str(PROJECT_ROOT)])
    return cmd


def _find_running_containers() -> list[str]:
    """Return container IDs running from the CI image."""
    result = subprocess.run(
        ["docker", "ps", "-q", "--filter", f"ancestor={IMAGE_NAME}"],
        capture_output=True, text=True,
    )
    return result.stdout.strip().split("\n") if result.stdout.strip() else []


def _kill_stale_containers():
    """Kill running CI containers and clear the lock file."""
    containers = _find_running_containers()
    if not containers:
        return
    console.print(f"[yellow]Killing {len(containers)} stale container(s)...[/yellow]")
    subprocess.run(["docker", "kill", *containers], capture_output=True)
    lock_path = TARGET_DIR / ".ci.lock"
    if lock_path.exists():
        lock_path.unlink()


PROFILE_TIMINGS_FILE = TARGET_DIR / ".profile_timings.json"


def _timed(label: str, func, *args, **kwargs):
    """Run *func* and return (result, elapsed_seconds)."""
    t0 = time.monotonic()
    result = func(*args, **kwargs)
    return result, time.monotonic() - t0


def build_image():
    """Build Docker image, relying on Docker layer cache to skip unchanged layers."""
    if not DOCKERFILE.exists():
        console.print(
            f"[red]Dockerfile not found: {DOCKERFILE.relative_to(PROJECT_ROOT)}[/red]"
        )
        sys.exit(1)
    run_cmd(_docker_build_cmd())


def _delegate_to_container(args: list[str], force: bool = False):
    """Build image and re-invoke ./ci inside the container.

    Acquires an exclusive file lock so only one container runs at a time.
    Checks for stale containers before starting and cleans up after.
    """
    build_image()
    TARGET_DIR.mkdir(exist_ok=True)

    # Pre-flight: check for stale containers
    stale = _find_running_containers()
    if stale:
        if force:
            _kill_stale_containers()
        else:
            console.print(
                "[red]Stale container(s) detected from a previous run.[/red]\n"
                "[red]Run with --force to kill them, or manually: "
                f"docker kill {' '.join(stale)}[/red]"
            )
            sys.exit(1)

    lock_path = TARGET_DIR / ".ci.lock"
    with open(lock_path, "w") as lock_fd:
        try:
            fcntl.flock(lock_fd, fcntl.LOCK_EX | fcntl.LOCK_NB)
        except OSError:
            console.print("[red]Another ./ci command is already running.[/red]")
            sys.exit(1)
        ci_cmd = "./ci " + " ".join(shlex.quote(a) for a in args)
        try:
            run_cmd([
                "docker", "run", "--rm",
                "-v", f"{PROJECT_ROOT}:/workspace:rw",
                "-w", "/workspace",
                IMAGE_NAME,
                ci_cmd,
            ])
        finally:
            # Post-run: verify container was cleaned up
            leftover = _find_running_containers()
            if leftover:
                console.print(
                    f"[yellow]Warning: {len(leftover)} container(s) still running "
                    f"after command completed. Cleaning up...[/yellow]"
                )
                _kill_stale_containers()


def _require_host(cmd_name: str):
    """Exit with error if running inside the container."""
    if IN_CONTAINER:
        console.print(f"[red]'{cmd_name}' can only be run from the host.[/red]")
        sys.exit(1)


def _clean_stale_bazel_symlinks():
    """Remove stale Bazel convenience symlinks that point to inaccessible locations."""
    for item in PROJECT_ROOT.iterdir():
        if item.is_symlink() and item.name.startswith("bazel-"):
            try:
                if item.resolve().exists():
                    continue
            except PermissionError:
                pass
            item.unlink()
            console.print(f"[dim]Removed stale symlink {item.name}[/dim]")


# ---------------------------------------------------------------------------
# CLI
# ---------------------------------------------------------------------------

@click.group()
@click.option("--force", is_flag=True, help="Kill stale containers before running.")
@click.pass_context
def cli(ctx, force):
    """Build and test tools for the Bazel test set rules project."""
    ctx.ensure_object(dict)
    ctx.obj["force"] = force


# ---------------------------------------------------------------------------
# Container commands — run directly when IN_CONTAINER, delegate otherwise
# ---------------------------------------------------------------------------

@cli.command()
@click.pass_context
def build(ctx):
    """Build all Bazel targets."""
    if not IN_CONTAINER:
        _delegate_to_container(["build"], force=ctx.obj["force"])
        return
    _clean_stale_bazel_symlinks()
    run_cmd(["bazel", "build", "//..."])


@cli.command()
@click.option("--profile", is_flag=True, help="Print wall-clock timing breakdown of each phase.")
@click.pass_context
def test(ctx, profile):
    """Run all Bazel tests and Python unit tests."""
    if not IN_CONTAINER:
        args = ["test"]
        if profile:
            args.append("--profile")
            # Remove stale timings so we don't show old data on failure
            PROFILE_TIMINGS_FILE.parent.mkdir(exist_ok=True)
            if PROFILE_TIMINGS_FILE.exists():
                PROFILE_TIMINGS_FILE.unlink()

            _, dt_build = _timed("docker_build", build_image)

            # Inline the container-run part of _delegate_to_container so we
            # can time it separately from the image build.
            force = ctx.obj["force"]
            stale = _find_running_containers()
            if stale:
                if force:
                    _kill_stale_containers()
                else:
                    console.print(
                        "[red]Stale container(s) detected. Run with --force to kill them.[/red]"
                    )
                    sys.exit(1)

            TARGET_DIR.mkdir(exist_ok=True)
            lock_path = TARGET_DIR / ".ci.lock"
            with open(lock_path, "w") as lock_fd:
                try:
                    fcntl.flock(lock_fd, fcntl.LOCK_EX | fcntl.LOCK_NB)
                except OSError:
                    console.print("[red]Another ./ci command is already running.[/red]")
                    sys.exit(1)
                ci_cmd = "./ci " + " ".join(shlex.quote(a) for a in args)
                try:
                    _, dt_container = _timed(
                        "container_run",
                        run_cmd,
                        [
                            "docker", "run", "--rm",
                            "-v", f"{PROJECT_ROOT}:/workspace:rw",
                            "-w", "/workspace",
                            IMAGE_NAME,
                            ci_cmd,
                        ],
                    )
                finally:
                    leftover = _find_running_containers()
                    if leftover:
                        _kill_stale_containers()

            # Read container-side timings and print combined summary
            import json as json_mod
            container_timings: list[tuple[str, float]] = []
            if PROFILE_TIMINGS_FILE.exists():
                container_timings = json_mod.loads(
                    PROFILE_TIMINGS_FILE.read_text()
                )

            console.print()
            console.print("[bold]===== Profile: wall-clock timing =====[/bold]")
            timings: list[tuple[str, float]] = [
                ("docker build (image)", dt_build),
            ]
            for label, dt in container_timings:
                timings.append((f"  {label}", dt))
            timings.append(("docker run  (total)", dt_container))
            for label, dt in timings:
                console.print(f"  {label:<40s} {dt:>7.1f}s")
            total = dt_build + dt_container
            console.print(f"  {'TOTAL':<40s} {total:>7.1f}s")
            return

        _delegate_to_container(args, force=ctx.obj["force"])
        return

    # --- Inside container ---
    timings: list[tuple[str, float]] = []

    _clean_stale_bazel_symlinks()
    _, dt = _timed("bazel test //...", run_cmd, ["bazel", "test", "//..."])
    timings.append(("bazel test //...", dt))

    console.print("[bold]--- Running pytest ---[/bold]")
    pytest_args = [
        "pytest", "orchestrator/", "ci_tool/", "tests/",
        "-v", "--tb=short",
    ]
    _, dt = _timed("pytest", run_cmd, pytest_args)
    timings.append(("pytest", dt))

    if profile:
        import json as json_mod
        PROFILE_TIMINGS_FILE.parent.mkdir(exist_ok=True)
        PROFILE_TIMINGS_FILE.write_text(json_mod.dumps(timings))


@cli.command()
@click.pass_context
def check(ctx):
    """Run type checks."""
    if not IN_CONTAINER:
        _delegate_to_container(["check"], force=ctx.obj["force"])
        return
    run_cmd([
        "mypy", "orchestrator/", "ci_tool/", "tests/",
        "--ignore-missing-imports",
    ])


@cli.command("test-examples")
@click.pass_context
def test_examples(ctx):
    """Run Bazel tests, lifecycle demo (burn-in/demotion/recovery), and reports."""
    if not IN_CONTAINER:
        _delegate_to_container(["test-examples"], force=ctx.obj["force"])
        return
    examples_dir = PROJECT_ROOT / "examples"
    for item in examples_dir.iterdir():
        if item.is_symlink() and item.name.startswith("bazel-"):
            try:
                if item.resolve().exists():
                    continue
            except PermissionError:
                pass
            item.unlink()
            console.print(f"[dim]Removed stale symlink examples/{item.name}[/dim]")

    status_path = examples_dir / "ecommerce" / ".tests" / "status"

    BURNIN_RUNS = 30
    DEGRADATION_RUNS = 20

    # Phase 1: Clean start + Bazel validation
    console.print("[bold]--- Phase 1: Clean start + Bazel validation ---[/bold]")
    shutil.rmtree(status_path.parent, ignore_errors=True)
    console.print("[dim]Removed .tests/ directory[/dim]")
    config_path = examples_dir / ".test_set_config"
    if config_path.exists():
        config_path.unlink()
        console.print("[dim]Removed .test_set_config[/dim]")
    reports_dir = examples_dir / "target" / "reports"
    if reports_dir.exists():
        shutil.rmtree(reports_dir)
        console.print("[dim]Removed target/reports/ directory[/dim]")
    run_cmd(["bazel", "test", "//..."], cwd=str(examples_dir))

    # Phase 2: Burn-in acceptance (all tests: new → burning_in → stable)
    # Run the orchestrator many times so the report accumulates a visible
    # timeline. All tests pass during this phase; SPRT accepts after ~28 runs.
    console.print(f"[bold]--- Phase 2: Burn-in acceptance ({BURNIN_RUNS} runs) ---[/bold]")
    _create_status_config(status_path)

    # First run populates the status file with "new" entries for all tests
    _run_orchestrator_quiet(examples_dir, status_path)
    console.print(f"[dim]  Run 1/{BURNIN_RUNS}[/dim]")

    # Transition all tests to burning_in so SPRT evaluates them
    _transition_all_to_burnin(status_path)

    for i in range(2, BURNIN_RUNS + 1):
        _run_orchestrator_quiet(examples_dir, status_path)
        if i % 10 == 0 or i == BURNIN_RUNS:
            console.print(f"[dim]  Run {i}/{BURNIN_RUNS}[/dim]")

    _print_lifecycle_summary(status_path, "after burn-in acceptance")

    # Phase 3: Degradation → demotion (email: stable → flaky)
    # Run with degraded email reliability so it fails sporadically.
    # SPRT detects the degradation and demotes email_notification to flaky.
    # The report timeline shows the sporadic failures accumulating.
    console.print(
        f"[bold]--- Phase 3: Degradation and demotion ({DEGRADATION_RUNS} runs) ---[/bold]"
    )
    for i in range(1, DEGRADATION_RUNS + 1):
        _run_orchestrator_quiet(
            examples_dir, status_path,
            env={"TST_EMAIL_RELIABILITY": "0.6"},
            check=False,
        )
        if i % 10 == 0 or i == DEGRADATION_RUNS:
            console.print(f"[dim]  Run {i}/{DEGRADATION_RUNS}[/dim]")

    _print_lifecycle_summary(status_path, "after degradation")

    # Phase 4: Generate reports for other examples
    console.print("[bold]--- Phase 4: Other example reports ---[/bold]")
    run_cmd(
        ["bazel", "run", "//macros_demo:deployment_validation", "--",
         "--allow-dirty"],
        cwd=str(examples_dir),
    )


def _create_status_config(status_path: Path) -> None:
    """Create .test_set_config at workspace root and empty status file."""
    import json as json_mod

    # Create .test_set_config at the workspace root (parent of ecommerce/)
    config_path = status_path.parent.parent.parent / ".test_set_config"
    config = {
        "min_reliability": 0.99,
        "statistical_significance": 0.95,
        "status_file": str(status_path),
    }
    config_path.write_text(json_mod.dumps(config, indent=2) + "\n")
    console.print("[dim]  Created .test_set_config[/dim]")

    # Create empty status file
    status = {"tests": {}}
    status_path.parent.mkdir(parents=True, exist_ok=True)
    status_path.write_text(json_mod.dumps(status, indent=2) + "\n")
    console.print("[dim]  Created status file[/dim]")


def _transition_all_to_burnin(status_path: Path) -> None:
    """Transition all 'new' tests in the status file to 'burning_in'.

    The orchestrator creates 'new' entries on first run but doesn't evaluate
    SPRT for them. Transitioning to 'burning_in' enables SPRT acceptance.
    """
    import json as json_mod

    data = json_mod.loads(status_path.read_text())
    count = 0
    for entry in data.get("tests", {}).values():
        if entry.get("state") == "new":
            entry["state"] = "burning_in"
            count += 1
    status_path.write_text(json_mod.dumps(data, indent=2) + "\n")
    console.print(f"[dim]  Transitioned {count} tests from new → burning_in[/dim]")


def _run_orchestrator_quiet(
    examples_dir: Path,
    status_path: Path,
    env: dict[str, str] | None = None,
    check: bool = True,
) -> subprocess.CompletedProcess[str]:
    """Run the ecommerce_tests orchestrator, suppressing output.

    Uses bazel run to invoke the orchestrator with status_file from config.
    Output is captured so the demo shows clean progress counters.
    """
    run_env = dict(os.environ)
    if env:
        run_env.update(env)
    return subprocess.run(
        [
            "bazel", "run", "//ecommerce:ecommerce_tests", "--",
            "--allow-dirty",
        ],
        cwd=str(examples_dir),
        env=run_env,
        check=check,
        capture_output=True,
        text=True,
    )


def _print_lifecycle_summary(status_path: Path, phase_label: str = "") -> None:
    """Print a summary of the status file after a lifecycle phase."""
    import json as json_mod

    header = "Lifecycle summary"
    if phase_label:
        header += f" ({phase_label})"
    console.print(f"[bold]{header}[/bold]")

    try:
        data = json_mod.loads(status_path.read_text())
    except (FileNotFoundError, json_mod.JSONDecodeError) as e:
        console.print(f"[red]Could not read status file: {e}[/red]")
        return

    tests = data.get("tests", {})
    state_counts: dict[str, int] = {}
    for name, entry in sorted(tests.items()):
        state = entry.get("state", "unknown")
        history = entry.get("history", [])
        runs = len(history)
        passes = sum(1 for h in history if h.get("passed", False))
        state_counts[state] = state_counts.get(state, 0) + 1
        short_name = name.split(":")[-1] if ":" in name else name
        if state == "stable":
            console.print(f"  [green]{short_name}: {state} ({runs} runs, {passes} passes)[/green]")
        elif state == "flaky":
            console.print(f"  [red]{short_name}: {state} ({runs} runs, {passes} passes)[/red]")
        elif state == "burning_in":
            console.print(f"  [yellow]{short_name}: {state} ({runs} runs, {passes} passes)[/yellow]")
        else:
            console.print(f"  [dim]{short_name}: {state} ({runs} runs)[/dim]")

    console.print()
    summary = ", ".join(f"{count} {state}" for state, count in sorted(state_counts.items()))
    console.print(f"[bold]Status summary: {summary}[/bold]")
    console.print()


@cli.command()
@click.argument("pattern", default="//...")
@click.pass_context
def query(ctx, pattern):
    """Query Bazel targets.

    PATTERN is a Bazel query expression (default: //...).
    Examples:
        ./ci query '//macros/examples:*'
        ./ci query 'kind(test, //...)'
    """
    if not IN_CONTAINER:
        _delegate_to_container(["query", pattern], force=ctx.obj["force"])
        return
    _clean_stale_bazel_symlinks()
    run_cmd(["bash", "-c", f"bazel query {shlex.quote(pattern)} 2>/dev/null | sort"])


# ---------------------------------------------------------------------------
# Host-only commands
# ---------------------------------------------------------------------------

@cli.command()
def shell():
    """Launch an interactive shell in the container."""
    _require_host("shell")
    build_image()
    run_cmd([
        "docker", "run", "--rm", "-it",
        "-v", f"{PROJECT_ROOT}:/workspace:rw",
        "-w", "/workspace",
        "--entrypoint", "bash",
        IMAGE_NAME,
    ])


@cli.command()
def clean():
    """Remove all generated files in target directory."""
    _require_host("clean")
    if TARGET_DIR.exists():
        shutil.rmtree(TARGET_DIR)
        TARGET_DIR.mkdir()
        console.print(
            f"[yellow]Cleaned {TARGET_DIR.relative_to(PROJECT_ROOT)}/[/yellow]"
        )
    else:
        TARGET_DIR.mkdir()
        console.print("[dim]Nothing to clean[/dim]")


@cli.command()
def kill():
    """Kill any stale CI containers."""
    _require_host("kill")
    containers = _find_running_containers()
    if not containers:
        console.print("[dim]No stale containers found.[/dim]")
        return
    _kill_stale_containers()
    console.print("[green]Stale containers killed.[/green]")


@cli.command("rebuild-image")
def rebuild_image():
    """Force rebuild Docker image."""
    _require_host("rebuild-image")
    if not DOCKERFILE.exists():
        console.print(
            f"[red]Dockerfile not found: {DOCKERFILE.relative_to(PROJECT_ROOT)}[/red]"
        )
        sys.exit(1)
    console.print(f"Rebuilding [bold]{IMAGE_NAME}[/bold]...")
    run_cmd(_docker_build_cmd(["--no-cache"]))
    console.print("[green]Image rebuilt[/green]")


if __name__ == "__main__":
    cli()
